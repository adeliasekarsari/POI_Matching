{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4083aac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Adelia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Adelia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.metrics import jaccard_score\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from shapely.geometry import Point\n",
    "from geopy.distance import geodesic\n",
    "import pandas as pd\n",
    "import string\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import geopandas as gpd\n",
    "# connection\n",
    "import os\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from dotenv import load_dotenv\n",
    "from shapely import Polygon, Point\n",
    "import glob\n",
    "import psycopg2\n",
    "from shapely.wkt import dumps\n",
    "\n",
    "load_dotenv(\"..\\.env\")\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb42fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(name):\n",
    "    inputs = tokenizer(name, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding\n",
    "\n",
    "def cal_cosine_similarity(name1, name2):\n",
    "    embedding1 = get_bert_embedding(name1)\n",
    "    embedding2 = get_bert_embedding(name2)\n",
    "    return torch.nn.functional.cosine_similarity(embedding1, embedding2).item()\n",
    "\n",
    "def fuzzy_similarity(name1, name2):\n",
    "    return fuzz.ratio(name1.lower(), name2.lower()) / 100.0\n",
    "\n",
    "def jaccard_similarity(name1, name2):\n",
    "    set1 = set(name1.lower().split())\n",
    "    set2 = set(name2.lower().split())\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def combined_similarity(name1, name2, weights=None):\n",
    "    if weights is None:\n",
    "        weights = [0.2, 0.45, 0.55]  # Adjust these weights as needed\n",
    "    fuzzy_sim = fuzzy_similarity(name1, name2)\n",
    "    jac_sim = jaccard_similarity(name1, name2)\n",
    "\n",
    "    combined_score = (\n",
    "        weights[1] * fuzzy_sim +\n",
    "        weights[2] * jac_sim\n",
    "    )\n",
    "    return combined_score\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "def remove_extra_whitespaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = to_lowercase(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_extra_whitespaces(text)\n",
    "    text = lemmatize_text(text)\n",
    "    return text\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def split_at_indices(s, index = 3):\n",
    "    return str(s[:index]) + '.' + str(s[index:])\n",
    "\n",
    "def calculate_distance(row, gdf):\n",
    "    point1 = (row.geometry.y, row.geometry.x)  # Original POI\n",
    "    point2 = (gdf.loc[gdf['id'] == row['id_buffer'], 'geometry'].iloc[0].y,\n",
    "              gdf.loc[gdf['id'] == row['id_buffer'], 'geometry'].iloc[0].x)  # Matched POI\n",
    "    return geodesic(point1, point2).meters\n",
    "\n",
    "def find_cluster(id, visited, cluster, group_mapping):\n",
    "    visited.add(id)\n",
    "    cluster.add(id)\n",
    "    for neighbor in group_mapping[id]:\n",
    "        if neighbor not in visited:\n",
    "            find_cluster(neighbor, visited, cluster, group_mapping)\n",
    "\n",
    "def tfidf_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return similarity[0][0]\n",
    "\n",
    "def levenshtein_similarity(text1, text2):\n",
    "    max_len = max(len(text1), len(text2))\n",
    "    return 1 - levenshtein_distance(text1, text2) / max_len\n",
    "\n",
    "def overlap_coefficient(text1, text2):\n",
    "    set1, set2 = set(text1.split()), set(text2.split())\n",
    "    return len(set1 & set2) / min(len(set1), len(set2))\n",
    "\n",
    "def dice_coefficient(text1, text2):\n",
    "    set1, set2 = set(text1.split()), set(text2.split())\n",
    "    return 2 * len(set1 & set2) / (len(set1) + len(set2))\n",
    "\n",
    "def inputData(match):\n",
    "    data3=','.join(\"'{0}'\".format(x) for x in match)\n",
    "    return data3\n",
    "\n",
    "def capitalize_without_punctuation(text):\n",
    "    punctuation_chars = set(string.punctuation)\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        if any(char in punctuation_chars for char in word):\n",
    "            processed_words.append(word)\n",
    "        else:\n",
    "            processed_words.append(word.capitalize())\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "\n",
    "def clean_df_input(df, poi_name_col, longitude_col, latitude_col):\n",
    "    \"\"\"\n",
    "    poi name, longitude, latitude\n",
    "    \"\"\"\n",
    "    df = df[[poi_name_col, longitude_col, latitude_col,'id']].drop_duplicates(subset = [poi_name_col, longitude_col, latitude_col])\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[longitude_col], \n",
    "                                                           df[latitude_col], \n",
    "                                                           crs = 'epsg:4326'))\n",
    "    buffer_distance_m = 30  # Distance in meters\n",
    "    buffer_user = gdf.copy()\n",
    "    buffer_user['geometry'] = buffer_user.buffer(buffer_distance_m / 109527.56)  # Convert meters to degrees\n",
    "    if gdf.shape[0]<=1:\n",
    "        return gdf, buffer_user\n",
    "    else:\n",
    "        buffered_gdf = gpd.sjoin(gdf, buffer_user[[poi_name_col, 'id', 'geometry']], how='inner', predicate='intersects')\n",
    "        buffered_gdf = buffered_gdf.rename(columns={\n",
    "            f'{poi_name_col}_left': poi_name_col, \n",
    "            f'{poi_name_col}_right': 'name_buffer',\n",
    "            'id_left': 'id',\n",
    "            'id_right': 'id_buffer'\n",
    "        })\n",
    "\n",
    "        buffered_gdf = buffered_gdf[~(buffered_gdf['id']==buffered_gdf['id_buffer'])]\n",
    "        buffered_gdf['distance'] = buffered_gdf.apply(lambda row: calculate_distance(row, gdf), axis=1)\n",
    "\n",
    "        similarity_threshold = 10  # Distance threshold in meters\n",
    "        buffered_gdf['is_similar'] = (\n",
    "            (buffered_gdf['distance'] <= similarity_threshold) &  # Distance condition\n",
    "            (buffered_gdf[poi_name_col].str.lower() == buffered_gdf['name_buffer'].str.lower())  # Name match condition\n",
    "        )\n",
    "\n",
    "        similar_pois = buffered_gdf[buffered_gdf['is_similar']]\n",
    "        similar_pois['pair'] = similar_pois.apply(\n",
    "            lambda row: tuple(sorted([row['id'], row['id_buffer']])), axis=1\n",
    "        )\n",
    "\n",
    "        group_mapping = defaultdict(set)\n",
    "        for _, row in similar_pois.iterrows():\n",
    "            group_mapping[row['id']].add(row['id'])\n",
    "            group_mapping[row['id']].add(row['id_buffer'])\n",
    "            group_mapping[row['id_buffer']].add(row['id'])\n",
    "\n",
    "        visited = set()\n",
    "        clusters = []\n",
    "        for id in group_mapping:\n",
    "            if id not in visited:\n",
    "                cluster = set()\n",
    "                find_cluster(id, visited, cluster, group_mapping)\n",
    "                clusters.append(cluster)\n",
    "\n",
    "        representative_ids = [min(cluster) for cluster in clusters]\n",
    "        representative_ids = representative_ids+gdf[~\n",
    "                                                    gdf['id'].isin(similar_pois['id'].unique().tolist())\n",
    "                                                   ]['id'].unique().tolist()\n",
    "\n",
    "        cleaned_gdf = gdf[gdf['id'].isin(representative_ids)].copy()\n",
    "        cleaned_gdf.reset_index(drop=True, inplace=True)\n",
    "        buffer_user = buffer_user[buffer_user['id'].isin(cleaned_gdf['id'].unique().tolist())]\n",
    "        return cleaned_gdf, buffer_user\n",
    "\n",
    "def query_poi_from_buffer(buffer_gdf, conn, industry_name = None):\n",
    "    # Ensure buffer_gdf is in WGS84 (EPSG:4326)\n",
    "    buffer_gdf = buffer_gdf.to_crs(epsg=4326)\n",
    "    \n",
    "    # Create a list of WKT strings for buffer geometries\n",
    "    buffer_wkt_list = buffer_gdf[\"geometry\"].apply(dumps).tolist()\n",
    "    \n",
    "    # Construct the SQL query with multiple polygons\n",
    "    polygon_conditions = \" OR \".join([\n",
    "        f\"ST_Intersects(a.geom, ST_GeomFromText('{polygon}', 4326))\"\n",
    "        for polygon in buffer_wkt_list\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    if industry_name !=None:\n",
    "        industry_name = inputData(industry_name)\n",
    "        filt = f\"\"\"AND e.industry_name in ({industry_name})\"\"\"\n",
    "    else :\n",
    "        filt = \"\"\n",
    "    sql = f\"\"\"\n",
    "    SELECT\n",
    "        a.poi_id,\n",
    "        a.poi_name,\n",
    "        a.geom,\n",
    "        e.industry_name\n",
    "    FROM v3_tbl_poi a\n",
    "    JOIN v3_tbl_brand b on a.brand_id = b.brand_id\n",
    "    JOIN v3_tbl_category c on b.category_id = c.category_id\n",
    "    JOIN v3_tbl_group d on c.group_id = d.group_id\n",
    "    JOIN v3_tbl_industry e on d.industry_id = e.industry_id\n",
    "    WHERE \n",
    "        {polygon_conditions} and a.status = 'T' and b.status = 'T' \n",
    "        and c.status = 'T' and d.status = 'T' and e.status = 'T' {filt};\n",
    "    \"\"\"\n",
    "        \n",
    "    return gpd.read_postgis(sql, conn, crs = 'epsg:4326')\n",
    "\n",
    "def clean_longest_sentence(text):\n",
    "    parts = text.split('|')\n",
    "    cleaned_parts = [re.sub(r'\\s*\\(.*?\\)\\s*', '', part).strip() for part in parts]\n",
    "    longest_sentence = max(cleaned_parts, key=len)\n",
    "    return longest_sentence\n",
    "\n",
    "def predict_poi(poi_name):\n",
    "    best_model = joblib.load(r\"poi_best_multioutput_model.pkl\")\n",
    "    vectorizer = joblib.load(r\"poi_tfidf_vectorizer.pkl\")\n",
    "    poi_name_tfidf = vectorizer.transform([poi_name])\n",
    "    predictions = best_model.predict(poi_name_tfidf)\n",
    "    return {\n",
    "        'brand_name': predictions[0][0],\n",
    "        'category_name': predictions[0][1],\n",
    "        'group_name': predictions[0][2],\n",
    "        'industry_name':predictions[0][3],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9938491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to production\n",
    "HOST = os.getenv('host_production')\n",
    "DB = os.getenv('DB_NAME')\n",
    "PORT = 5432\n",
    "USER= os.getenv('username_production')\n",
    "PWD = os.getenv('password_production')\n",
    "conn0 = psycopg2.connect(host=HOST,database=DB, user=USER, password=PWD)\n",
    "\n",
    "DB_NAME_V3_POI=os.getenv(\"DB_NAME_V3_POI\")\n",
    "DB_USERNAME_V3_POI=os.getenv(\"DB_USERNAME_V3_POI\")\n",
    "DB_PASSWORD_V3_POI=os.getenv(\"DB_PASSWORD_V3_POI\")\n",
    "DB_HOST_V3=os.getenv(\"DB_HOST_V3\")\n",
    "conn_poi = psycopg2.connect(host=DB_HOST_V3,\n",
    "                            database=DB_NAME_V3_POI, \n",
    "                            user=DB_USERNAME_V3_POI, \n",
    "                            password=DB_PASSWORD_V3_POI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53142ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"data_raw.xlsx\")\n",
    "df['id'] = range(0, len(df))\n",
    "df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2405d6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adelia\\AppData\\Local\\Temp\\ipykernel_4024\\1567011507.py:118: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  buffer_user['geometry'] = buffer_user.buffer(buffer_distance_m / 109527.56)  # Convert meters to degrees\n"
     ]
    }
   ],
   "source": [
    "gdf, buffer_gdf = clean_df_input(df=df, poi_name_col='nama_merchant', longitude_col= 'longitude',latitude_col='latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54893ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gdf.copy()\n",
    "buffer = buffer_gdf[buffer_gdf['id'].isin(data['id'].unique().tolist())].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bebbe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 222 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "poi_internal = gpd.read_parquet(\"data_internal.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67d70b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_join = gpd.sjoin(\n",
    "    poi_internal,\n",
    "    buffer[['geometry','nama_merchant','id']]\n",
    "         ).drop(columns = 'index_right')\n",
    "# poi_not_similar = data[~data['id'].isin(poi_join['id'].unique().tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a8df7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = poi_join.rename(columns = {'geom':'geometry_internal'})\n",
    "x = pd.merge(x, data[['id','geometry']].rename(columns = {'geometry':'geometry_input'}), on = 'id')\n",
    "\n",
    "# Convert to a CRS that uses meters (e.g., EPSG:3395)\n",
    "x['geometry_internal'] = x['geometry_internal'].to_crs(epsg=3395)\n",
    "x['geometry_input'] = x['geometry_input'].to_crs(epsg=3395)\n",
    "\n",
    "# Calculate the distance in meters\n",
    "x['distance_meters'] = x['geometry_internal'].distance(x['geometry_input'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c64b006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 93.8 ms\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean the 'poi_name' and 'nama_merchant' columns using the preprocess_text function\n",
    "x['cleaned_poi_name'] = x['poi_name'].apply(preprocess_text)\n",
    "x['cleaned_nama_merchant'] = x['nama_merchant'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a49c22f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 328 ms\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate the similarity\n",
    "df = x.copy()\n",
    "df['similarity'] = df.apply(lambda row: combined_similarity(row['cleaned_poi_name'], row['cleaned_nama_merchant']), axis=1)\n",
    "df['fuzzy'] = df.apply(lambda row: fuzzy_similarity(row['cleaned_poi_name'], row['cleaned_nama_merchant']), axis=1)\n",
    "df['jaccard'] = df.apply(lambda row:jaccard_similarity(row['cleaned_poi_name'], row['cleaned_nama_merchant']), axis=1)\n",
    "df['tfidf_similarity'] = df.apply(lambda row:tfidf_similarity(row['cleaned_poi_name'], row['cleaned_nama_merchant']), axis=1)\n",
    "df['levenshtein_similarity'] = df.apply(lambda row:levenshtein_similarity(row['cleaned_poi_name'], row['cleaned_nama_merchant']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba55ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = ['distance_meters','fuzzy','jaccard','tfidf_similarity','levenshtein_similarity']\n",
    "df_predict = df[x_columns+['poi_id','id']]\n",
    "X_test = df_predict[x_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "356b7dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickled_model = pickle.load(open('model_v1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e948244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adelia\\AppData\\Local\\Temp\\ipykernel_21024\\2077780276.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_predict['target_model'] = predict\n"
     ]
    }
   ],
   "source": [
    "predict = pickled_model.predict(X_test)\n",
    "df_predict['target_model'] = predict\n",
    "df['target_model'] = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "991d39bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adelia\\anaconda3\\envs\\geo_env\\Lib\\site-packages\\geopandas\\geodataframe.py:1443: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "similar = df[df['target_model']==1]\n",
    "similar['similarity'] = similar[['fuzzy','jaccard','tfidf_similarity','levenshtein_similarity']].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "555d6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = similar[(similar['fuzzy']>0.775)&(similar['tfidf_similarity']>0.5)&\n",
    "        (similar['jaccard']>0.5)&(similar['levenshtein_similarity']>0.667)]\n",
    "not_ = similar[~(similar['id'].isin(first['id'].tolist()))]\n",
    "second = not_[not_['similarity']>0.64]\n",
    "\n",
    "not_2 = not_[~(not_['id'].isin(second['id'].tolist()))]\n",
    "third = not_2[not_2['fuzzy']>0.9]\n",
    "\n",
    "not_3 = not_2[~(not_2['id'].isin(third['id'].tolist()))]\n",
    "fourth = not_3[(not_3['similarity']>0.6)&(not_3['tfidf_similarity']>0.6)]\n",
    "\n",
    "not_4 = not_3[~not_3['id'].isin(fourth['id'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "395b894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = pd.concat([first, second, third, fourth]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47a9c789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adelia\\anaconda3\\envs\\geo_env\\Lib\\site-packages\\geopandas\\geodataframe.py:1443: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "gdf_sorted = similar.sort_values(by=['id', 'similarity', 'distance_meters'], ascending=[True, False, True])\n",
    "\n",
    "# Drop duplicates, keeping the first occurrence (the one with max similarity and min distance)\n",
    "gdf_unique = gdf_sorted.drop_duplicates(subset='id', keep='first')\n",
    "poi_similar = gdf_unique[['poi_id','poi_name','nama_merchant',\n",
    "                          'id','distance_meters','similarity',\n",
    "                          'fuzzy','jaccard','geometry_internal']]\n",
    "\n",
    "poi_similar['nama_merchant'] = poi_similar['nama_merchant'].apply(capitalize_without_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "395a83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = df_raw[['nama_merchant',\n",
    " 'latitude',\n",
    " 'longitude','id']]\n",
    "final = final[~(final['id'].isin(poi_similar['id'].tolist()))]\n",
    "final['poi_name'] = final['nama_merchant'].apply(capitalize_without_punctuation)\n",
    "final['poi_name'] = final['poi_name'].apply(clean_longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded = final['poi_name'].apply(predict_poi).apply(pd.Series)\n",
    "poi_non_similar = pd.concat([final, df_expanded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84335ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_non_similar.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c24745",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_similar.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
