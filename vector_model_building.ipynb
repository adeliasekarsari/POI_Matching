{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "602e8f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connection\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from shapely import Polygon, Point\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "load_dotenv(\"..\\..\\.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce44190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to production\n",
    "HOST = os.getenv('host_production')\n",
    "DB = os.getenv('DB_NAME')\n",
    "PORT = 5432\n",
    "USER= os.getenv('username_production')\n",
    "PWD = os.getenv('password_production')\n",
    "conn0 = psycopg2.connect(host=HOST,database=DB, user=USER, password=PWD)\n",
    "\n",
    "DB_NAME_V3_POI=os.getenv(\"DB_NAME_V3_POI\")\n",
    "DB_USERNAME_V3_POI=os.getenv(\"DB_USERNAME_V3_POI\")\n",
    "DB_PASSWORD_V3_POI=os.getenv(\"DB_PASSWORD_V3_POI\")\n",
    "DB_HOST_V3=os.getenv(\"DB_HOST_V3\")\n",
    "conn_poi = psycopg2.connect(host=DB_HOST_V3,\n",
    "                            database=DB_NAME_V3_POI, \n",
    "                            user=DB_USERNAME_V3_POI, \n",
    "                            password=DB_PASSWORD_V3_POI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT\n",
    "    a.poi_id,\n",
    "    a.poi_name,\n",
    "    b.brand_name,\n",
    "    c.category_name,\n",
    "    d.group_name,\n",
    "    e.industry_name\n",
    "FROM v3_tbl_poi a\n",
    "JOIN v3_tbl_brand b on a.brand_id = b.brand_id\n",
    "JOIN v3_tbl_category c on b.category_id = c.category_id\n",
    "JOIN v3_tbl_group d on c.group_id = d.group_id\n",
    "JOIN v3_tbl_industry e on d.industry_id = e.industry_id\n",
    "JOIN v3_admin f on a.kode_desa = f.kode_desa\n",
    "WHERE a.status = 'T' and b.status = 'T' and c.status = 'T' and d.status = 'T' and e.status = 'T'\n",
    "    and f.status = 'T' and f.id_source = 6 and nama_kota = 'JAKARTA PUSAT'\n",
    "\"\"\"\n",
    "df = pd.read_sql(sql, conn_poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Data\n",
    "# df = pd.read_csv('poi_data.csv')  # Replace with your actual dataset file\n",
    "X = df['poi_name']  # Input: POI name\n",
    "y = df[['brand_name','category_name', 'group_name', 'industry_name']]  # Targets\n",
    "\n",
    "# Step 2: Data Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Text Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Step 4: Define Model and Hyperparameter Tuning\n",
    "base_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [50, 100, 150],\n",
    "    'estimator__max_depth': [None, 10, 20],\n",
    "    'estimator__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "multi_output_model = MultiOutputClassifier(base_model)\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=multi_output_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',  # You can use other metrics such as f1_macro\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model with hyperparameter tuning\n",
    "print(\"Starting model tuning...\")\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Best model after tuning\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Step 5: Model Testing\n",
    "y_pred = best_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model for each target\n",
    "for i, target in enumerate(['category_name', 'industry_name', 'group_name']):\n",
    "    print(f\"\\nClassification Report for {target}:\")\n",
    "    print(classification_report(y_test.iloc[:, i], y_pred[:, i]))\n",
    "\n",
    "# Step 6: Save the Best Model and TF-IDF Vectorizer\n",
    "joblib.dump(best_model, 'poi_best_multioutput_model.pkl')\n",
    "joblib.dump(vectorizer, 'poi_tfidf_vectorizer.pkl')\n",
    "print(\"Model and vectorizer saved successfully!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
